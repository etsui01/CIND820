---
title: "CIND820 Physical Health and Mental Health"
author: "Erica Tsui 501073613"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Install packages for pre-processing and classification models.
```{r}
#install.packages("tidyverse")
#install.packages("haven")
#install.packages("rpart")
#install.packages("rpart.plot")
#install.packages("rattle")
#install.packages("caret")
#install.packages("corrplot")
#install.packages("ROSE")
#install.packages("naivebayes")
#install.packages("randomForest")
#install.packages("MASS")
#install.packages("knitr")
library(MASS)
library(tidyverse)
library(haven)
library(rpart)
library(rpart.plot)
library(rattle)
library(caret)
library(corrplot)
library(ROSE)
library(naivebayes)
library(randomForest)
library(knitr)
```
Import the BRFSS (Behavioral Risk Factor Surveillance System) 2019 dataset. The original file was in a SAS .xpt format but will be saved as a .csv after processing. <br/>
The file is in a zip file located at https://www.cdc.gov/brfss/annual_data/2019/files/LLCP2019XPT.zip
```{r}
LLCP_2019 <- read_xpt(file = "D:/School/CIND820/LLCP2019.xpt")
```
Extract the desired variables and rename them with more intuitive labels and to make them more readable by R.
```{r}
BRFSS_raw <- LLCP_2019 %>%
  dplyr::select(
    MENTHLTH, `_AGE80`, `_RACE`, `_SEX`, `_BMI5`, `CHILDREN`,
    `_EDUCAG`, `INCOME2`, TRNSGNDR, SOMALE, SOFEMALE, MARITAL,
    EMPLOY1, GENHLTH, PHYSHLTH, `_SMOKER3`, `_DRNKWK1`, `_FRUTSU1`, `_VEGESU1`,
    `_PA150R3`, `_RFHYPE5`, `_RFCHOL2`, `_ASTHMS1`, `_DRDXAR2`
    ) %>%
  rename(
    mental_health = MENTHLTH,
    age = `_AGE80`,
    race = `_RACE`,
    sex = `_SEX`,
    bmi = `_BMI5`,
    children = `CHILDREN`,
    education = `_EDUCAG`,
    income = `INCOME2`,
    transgender = TRNSGNDR,
    sexorient_male = SOMALE,
    sexorient_fem = SOFEMALE,
    marital = MARITAL,
    employment = EMPLOY1,
    gen_health = `GENHLTH`,
    bad_health_days = `PHYSHLTH`, 
    smoker = `_SMOKER3`,
    drinks_week = `_DRNKWK1`,
    fruit_day = `_FRUTSU1`,
    vegetable_day = `_VEGESU1`,
    exercise_150 = `_PA150R3`,
    blood_pres = `_RFHYPE5`,
    cholesterol = `_RFCHOL2`,
    asthma = `_ASTHMS1`,
    arthritis = `_DRDXAR2`
  )
```
For each variable the responses were tallied to identify NA, "refused" or "don't know" responses. The "don't know/refused" responses were coded 7/77 or 9/99/99900 depending on the variable. Any rows having these three response types were removed before further data exploration. There was some inconsistency with how valid zero responses were recorded. Variables that coded 0 as '88' were corrected to avoid R treating the response as a numerical 88. 
```{r}
summary(BRFSS_raw)

#Repeat below for every variable
c <- count(BRFSS_raw, mental_health)
view(c)
```
Every variable was cleaned for the three responses mentioned above. Most of the original variables were coded numerically instead of the named categories and when the dataset was imported all the classes were defaulted to numerical. Variables had their numerical codes replaced with the actual names and set to the correct ordinal or nominal classes if appropriate. Outliers were defined as being more than 3 standard deviations away from the mean (after cleaning and adjusting '88' responses) and removed. Most outliers only fell on the right boundary since the left would have been in the negatives and impossible (ex. negative fruit consumption). Some variable categories were combined. The transgender and sexual orientation variables were removed completely due to the high number of NA results.
```{r}
#Change mental health days from numerical to Yes/No
BRFSS <- BRFSS_raw %>%
  filter(!(is.na(mental_health) | mental_health %in% c(77,99))) %>%
  mutate(mental_health = factor(ifelse(mental_health == 88, "No", "Yes"), 
                                ordered = FALSE))

#Change race values to unordered names
#Combine Hawaiians (5) into Other (6)
BRFSS <- BRFSS %>%
  filter(!(is.na(race) | race == 9)) %>%
  mutate(race = factor(case_when(race == 1 ~ "White",
                                 race == 2 ~ "Black",
                                 race == 3 ~ "Indigenous",
                                 race == 4 ~ "Asian",
                                 race %in% c(5, 6) ~ "Other",
                                 race == 7 ~ "Multiracial",
                                 race == 8 ~ "Hispanic"),
                       ordered = FALSE))

#Re-label sex to male and female
BRFSS <- BRFSS %>%
  mutate(sex = factor(ifelse(sex == 1, "Male", "Female"), ordered = FALSE))

#Calculating BMI outliers then removing
bmi_sd <- sd(BRFSS$bmi, na.rm = TRUE)
bmi_mean <- mean(BRFSS$bmi, na.rm = TRUE)
bmi_out_lower <- bmi_mean - (3 * bmi_sd)
bmi_out_upper <- bmi_mean + (3 * bmi_sd)

BRFSS <- BRFSS %>%
  filter(!(is.na(bmi)) & (bmi > bmi_out_lower) & (bmi < bmi_out_upper))

#Re-label children entry '88' to 0 children
BRFSS <- BRFSS %>%
  filter(!(is.na(children) | children == 99)) %>%
  mutate(children = replace(children, children == 88, 0))

#Calculating outliers, rounded up for whole children
child_sd <- sd(BRFSS$children)
child_mean <- mean(BRFSS$children)
child_out_upper <- round(child_mean + (3 * child_sd)) 

#Removing right side children outliers, no left needed since no negative kids
BRFSS <- BRFSS[BRFSS$children <= child_out_upper,]

#Set education to ordinal levels and names
BRFSS <- BRFSS %>%
  filter(!(is.na(education) | education == 9)) %>%
  mutate(education = factor(case_when(education == 1 ~ "Some High School",
                                      education == 2 ~ "High School",
                                      education == 3 ~ "Some Post Secondary",
                                      education == 4 ~ "Post Secondary"),
                            levels = c("Some High School", "High School",
                                       "Some Post Secondary", "Post Secondary"),
                            ordered = TRUE))

#Set income to ordinal level bins
BRFSS <- BRFSS %>%
  filter(!(is.na(income) | income %in% c(77,99))) %>%
  mutate(income = factor(case_when(income == 1 ~ "<10",
                                   income == 2 ~ "10-15",
                                   income == 3 ~ "15-20",
                                   income == 4 ~ "20-25",
                                   income == 5 ~ "25-35",
                                   income == 6 ~ "35-50",
                                   income == 7 ~ "50-75",
                                   income == 8 ~ ">75"), 
                         levels = c("<10", "10-15", "15-20", "20-25",
                                    "25-35", "35-50", "50-75", ">75"),
                         ordered = TRUE))

#Remove transgender variable due to high NAs
paste("Transgender variable NA sum: ", sum(is.na(BRFSS$transgender)))
BRFSS$transgender <- NULL

#Combine sex orientation of males and females into a new column
BRFSS <- BRFSS %>%
  mutate(orientation = coalesce(sexorient_male, sexorient_fem))
#Checking how many NA values exist after combining genders into one column
paste("Sexual orientation variable NA sum: ", sum(is.na(BRFSS$orientation)))
#Removing all sexual orientation columns due to high NAs
BRFSS$sexorient_male <- NULL
BRFSS$sexorient_fem <- NULL
BRFSS$orientation <- NULL

#Re-label marital status to nominal titles
BRFSS <- BRFSS %>%
  filter(!(is.na(marital) | marital == 9)) %>%
  mutate(marital = factor(case_when(marital == 1 ~ "Married",
                                    marital == 2 ~ "Divorced",
                                    marital == 3 ~ "Widowed",
                                    marital == 4 ~ "Separated",
                                    marital == 5 ~ "Never Married",
                                    marital == 6 ~ "Unmarried Couple"),
                          ordered = FALSE))

#Re-label employment, merge "Employed for wages" with "Self-employed"
#Merge all unemployment categories into single "Unemployed"
BRFSS <- BRFSS %>%
  filter(!(is.na(employment) | employment == 9)) %>%
  mutate(employment = factor(case_when(employment %in% c(1, 2) ~ "Employed",
                                       employment %in% c(3:6) ~ "Unemployed",
                                       employment == 7 ~ "Retired",
                                       employment == 8 ~ "Unable to Work"),
                             ordered = FALSE))

#Re-label general health status and set ordinal levels
BRFSS <- BRFSS %>%
  filter(!(is.na(gen_health) | gen_health %in% c(7,9))) %>%
  mutate(gen_health = factor(case_when(gen_health == 1 ~ "Excellent",
                                       gen_health == 2 ~ "Very Good",
                                       gen_health == 3 ~ "Good",
                                       gen_health == 4 ~ "Fair",
                                       gen_health == 5 ~ "Poor"), 
                             levels = c("Poor", "Fair", "Good", "Very Good",
                                        "Excellent"),
                             ordered = TRUE))

#Set bad_health_days response '88' to 0 days
BRFSS <- BRFSS %>%
  filter(!(is.na(bad_health_days) | bad_health_days %in% c(77,99))) %>%
  mutate(bad_health_days = replace(bad_health_days, bad_health_days == 88, 0))

#Re-label smoking status
BRFSS <- BRFSS %>%
  filter(!(is.na(smoker) | smoker == 9)) %>%
  mutate(smoker = factor(case_when(smoker %in% c(1, 2) ~ "Current Smoker",
                                   smoker == 3 ~ "Former Smoker",
                                   smoker == 4 ~ "Never Smoked"),
                         ordered = FALSE))

#Remove drinks per week outliers (calculated after removing '99900' responses)
BRFSS <- BRFSS[BRFSS$drinks_week != 99900,]

drink_sd <- sd(BRFSS$drinks_week)
drink_mean <- mean(BRFSS$drinks_week)
drink_out_upper <- drink_mean + (3 * drink_sd) #No lower bound needed
BRFSS <- BRFSS[BRFSS$drinks_week <= drink_out_upper,]

#Remove fruits per day and vegetables per day outliers
fruit_sd <- sd(BRFSS$fruit_day, na.rm = TRUE)
fruit_mean <- mean(BRFSS$fruit_day, na.rm = TRUE)
fruit_out_upper <- fruit_mean + (3 * fruit_sd) #No lower bound needed

veg_sd <- sd(BRFSS$vegetable_day, na.rm = TRUE)
veg_mean <- mean(BRFSS$vegetable_day, na.rm = TRUE)
veg_out_upper <- veg_mean + (3 * veg_sd) #No lower bound needed

BRFSS <- BRFSS %>%
  filter(!(is.na(fruit_day) | fruit_day > fruit_out_upper)) %>%
  filter(!(is.na(vegetable_day) | vegetable_day > veg_out_upper))

#Re-label exercise time ranges and set ordinal factor levels
BRFSS <- BRFSS %>%
  filter(!(is.na(exercise_150) | exercise_150 == 9)) %>%
  mutate(exercise_150 = factor(case_when(exercise_150 == 1 ~ ">150",
                                         exercise_150 == 2 ~ "1-149",
                                         exercise_150 == 3 ~ "0",), 
                               levels = c("0", "1-149", ">150"),
                               ordered = TRUE))

#Re-label high blood pressure to Yes and No
BRFSS <- BRFSS %>%
  filter(!(is.na(blood_pres) | blood_pres == 9)) %>%
  mutate(blood_pres = factor(ifelse(blood_pres == 1, "No", "Yes"), 
                             ordered = FALSE))

#Re-label high cholesterol to Yes and No
BRFSS <- BRFSS %>%
  filter(!(is.na(cholesterol) | cholesterol == 9)) %>%
  mutate(cholesterol = factor(ifelse(cholesterol == 1, "No", "Yes"), 
                              ordered = FALSE))

#Re-label asthma status
BRFSS <- BRFSS %>%
  filter(!(is.na(asthma) | asthma == 9)) %>%
  mutate(asthma = factor(case_when(asthma == 1 ~ "Current",
                                   asthma == 2 ~ "Former",
                                   asthma == 3 ~ "Never"), 
                         ordered = FALSE))

#Re-label arthritis to Yes and No
BRFSS <- BRFSS %>%
  filter(!(is.na(arthritis))) %>%
  mutate(arthritis = factor(ifelse(arthritis == 1, "Yes", "No"), 
                            ordered = FALSE))

write.csv(BRFSS, "BRFSS.csv")
```
There are some highlights looking at the distribution of the individual variables. Age was skewed left more than expected since the range of ages recorded was 18 to 80 putting the theoretical mean at 31. This could have been caused by the method that responses were gathered. Since all interviews were conducted over the phone (landline and cellphone) younger people may have screened the unknown number and not picked up. The disproportionate age may make the model less effective for younger people. <br/>

Race and BMI supported the dataset reflecting the US population. The large number of White respondents was close to the US Census reported 76%. The CDC says a normal BMI is 18 - 25 which was close to the peak of the BMI distribution. <br/>

Most respondents rated their general physical health as good or very good but not the highest rating of excellent. Since it was a self-rating there was the possibility that a mentally unwell person may perceive their physical health differently than a mentally healthy person. A related numerical variable was the number of bad physical health days in the last 30 days. More than half of the respondents said they experienced 0 bad days. Interestingly, the distribution of those who experienced some bad days dipped in the middle. It seemed people either experienced very short term problems or had chronic conditions but few reported moderate length problems. 
```{r}
barchart(BRFSS$mental_health, main = "Bad mental health in last 30 Days")
hist(BRFSS$age, main = 'Age', xlab = 'Age')
barchart(BRFSS$race, main = "Race")
barchart(BRFSS$sex, main = "sex")
hist(BRFSS$bmi, main = "Body Mass Index", xlab = "BMI (2 implied decimals)")
hist(BRFSS$children, main = "Children in Household", 
     xlab="children in household")
barchart(BRFSS$education, main = "Highest Education Attained")
barchart(BRFSS$income, main = "Income (in $1000s)")
barchart(BRFSS$marital, main = "Marital Status")
barchart(BRFSS$employment, main = "Employment")
barchart(BRFSS$gen_health, main = "Self-rating of General Health")
hist(BRFSS$bad_health_days, main = "Bad physical health days (in last 30)",
     xlab = "Bad physical health days")
barchart(BRFSS$smoker, main = "Smoker Status")
hist(BRFSS$drinks_week, main = "Drinks per Week", xlab = "Drinks per week")
hist(BRFSS$fruit_day, main = "Fruit Consumed", 
     xlab = "Fruit consumed (implied 2 decimals)")
hist(BRFSS$vegetable_day, main = "Vegetables Consumed", 
     xlab = "Vegetables consumed (implied 2 decimals)")
```
The below correlation matrix and visualization shows the correlation for most the independent variables were not very high. The two exceptions being: <br/>
1) The moderate negative correlation between the respondent's age and children in the household. As age increased the number of children in the household decreased. <br/>
2) The weak positive correlation between fruit and vegetables consumed. It was expected that drinks per week would have a moderate negative correlation with fruits and vegetables but the matrix showed little relationship. <br/>
Unfortunately some categorical variables did not have numerical data and could not be included in the correlation matrix. The income variable in particular would have been good to include but only had range responses instead of numerical.
```{r}
BRFSS_num <- BRFSS %>%
  dplyr::select(age, bmi, children, bad_health_days, drinks_week, fruit_day, 
                vegetable_day)

BRFSS_num_corr <- cor(BRFSS_num)
corrplot(BRFSS_num_corr, method = "circle")
BRFSS_num_corr
```
The classification models used are: <br/>
1) Decision Tree <br/>
2) Random Forest <br/>
3) Naive Bayes <br/>
4) Logistic Regression <br/>
To prepare, the dataset was split into a train and test group. The ratio was 70:30 train to test. 
```{r}
set.seed(10)
test_index <- sample(1:nrow(BRFSS), nrow(BRFSS) * 0.3)
train_set <- as.data.frame(BRFSS[-test_index, ])
test_set <- as.data.frame(BRFSS[test_index, ])
```
The dependent mental_health variable is imbalanced with 65% 'No' and 35% 'Yes'. This was balanced using ROSE (Random Over-Sampling Examples) to generate more 'Yes' responses. This was only done on the training set since artificially created records in the test set would negatively affect the results. Unfortunately, The ROSE function only accepts numeric and unordered factors so the ordering was removed from the applicable variables. 
```{r}
#Remove factor ordering for balanced set
train_set_bal <- train_set
train_set_bal$education <- factor(train_set_bal$education, ordered = FALSE)
train_set_bal$income <- factor(train_set_bal$income, ordered = FALSE)
train_set_bal$gen_health <- factor(train_set_bal$gen_health, ordered = FALSE)
train_set_bal$exercise_150 <- factor(train_set_bal$exercise_150, ordered = FALSE)

#Balancing with ROSE and checking ratio
kable(prop.table(table(train_set$mental_health)), "simple",
      caption = "Mental health response variable before balancing",
      col.names = c(" ", "Frequency")) #65% No to 35% Yes
#Calc min size needed to balance
total_size_need <- table(train_set_bal$mental_health)[[1]] * 2
train_set_bal <- ROSE(mental_health ~., data = train_set_bal, 
                    N = total_size_need, p = 0.5)$data

#Confirming balance
kable(prop.table(table(train_set_bal$mental_health)), "simple",
      caption = "Mental health response variable after balancing",
      col.names = c(" ", "Frequency")) #50% No to 50% Yes
```

K-fold validation was used for Decision Tree, Naive Bayes, and Logistic Regression but not Random Forest. It was left at the default 10 folds. Since the models all used the same test set to predict and compare results, k-fold was only applied to the training set after balancing. It would have been too computationally expensive to balance each training fold as it was used.
```{r}
set.seed(10)
k_fold_train <- trainControl(method = "cv", number = 10)
```
Decision Tree Classifier - Default <br/>
For the first run the function was left completely default with all the variables. The resulting tree only split twice and the terminal nodes had a high impurity. Specificity was inflated because the model was rejecting more cases as no risk. The biggest issue was the low recall even though precision was also low (confirmed by the low F1 score). These combined measurements showed that this model was missing at risk people. 
```{r}
train_tree_default <- rpart(mental_health ~ ., data = train_set, method = "class")
summary(train_tree_default)
fancyRpartPlot(train_tree_default)
test_tree_pred <- predict(train_tree_default, newdata = test_set, type = "class")
tree_table_default <- table(Predicted = factor(test_tree_pred, levels = c("Yes", "No")), 
                            Actual = factor(test_set$mental_health, levels = c("Yes", "No")))
tree_cm_default <- confusionMatrix(tree_table_default, positive = "Yes")
tree_cm_default
tree_cm_default$byClass[7] #F1-score isn't included in summary
```
Decision Tree Classifier - Balanced Training Set
This tree split more and added to the features being used. It still used bad health days and age but chose different points and added marital status and sex.
```{r}
train_tree_bal <- rpart(mental_health ~ ., data = train_set_bal, method = "class")
fancyRpartPlot(train_tree_bal)

test_tree_pred <- predict(train_tree_bal, newdata = test_set, type = "class")
tree_table_bal <- table(Predicted = factor(test_tree_pred, 
                                           levels = c("Yes", "No")), 
                        Actual = factor(test_set$mental_health, 
                                        levels = c("Yes", "No")))
tree_cm_bal <- confusionMatrix(tree_table_bal, positive = "Yes")
tree_cm_bal
tree_cm_bal$byClass[7]
```
Decision Tree - Balanced Training Set and k-fold
Multiple folds increased performance but restricted the tree. Even though it has the same number of splits with the same variables as the first tree the point of splitting is different. 
```{r}
train_tree_kf <- train(mental_health ~., data = train_set_bal, 
                       trControl = k_fold_train,
                       method = "rpart")
fancyRpartPlot(train_tree_kf$finalModel)

test_tree_kf_pred <- predict(train_tree_kf, newdata = test_set)
tree_table_kf <- table(predicted = factor(test_tree_kf_pred, levels = c("Yes", "No")), 
                   actual = factor(test_set$mental_health, levels = c("Yes", "No")))
tree_cm_kf <- confusionMatrix(tree_table_kf, positive = "Yes")
tree_cm_kf
tree_cm_kf$byClass[7]
```

Comparing the Decision Trees <br/>
The Decision Tree generated with the balanced dataset and k-fold validation performed best. Despite the small increase in balanced accuracy, there was a large increase in recall relative to the small decrease in precision. This desirable trade-off lead to an increase in the F1-score. Detection rate increased as the balanced dataset model was better at identifying true positives. Comparing the trees the second tree is the largest yet does not predict new data as well as the final tree. The final k-fold tree shrunk back to the same two variables as the initial tree but the point that each node was split was different. The decision tree seems to make generalized decisions using only bad health and age.
```{r}
kable(data.frame(Models = c("Default", "Balanced", "Balanced & k-fold"),
                 'F1-score' = round(c(tree_cm_default$byClass[7], 
                                      tree_cm_bal$byClass[7],
                                      tree_cm_kf$byClass[7]), 5),
                 Recall = round(c(tree_cm_default$byClass[6], 
                                  tree_cm_bal$byClass[6],
                                  tree_cm_kf$byClass[6]), 5),
                 Precision = round(c(tree_cm_default$byClass[5], 
                                     tree_cm_bal$byClass[5],
                                     tree_cm_kf$byClass[5]), 5),
                 'Detection Rate' = round(c(tree_cm_default$byClass[9], 
                                            tree_cm_bal$byClass[9],
                                            tree_cm_kf$byClass[9]), 5),
                 'Balanced Accuracy' = round(c(tree_cm_default$byClass[11], 
                                               tree_cm_bal$byClass[11],
                                               tree_cm_kf$byClass[11]), 5)),
      caption = "Decision Trees",
      format = "simple")
```

Random Forest - Default <br/>
Random Forest was added as another model to try and improve upon the results generated by the Decision Tree models. The function by default used 500 trees.
```{r}
rf_model_default <- randomForest(mental_health ~., data = train_set)
test_forest_pred <- predict(rf_model_default, newdata = test_set)
forest_table_default <- table(Predicted = factor(test_forest_pred, 
                                                 levels = c("Yes", "No")), 
                              Actual = factor(test_set$mental_health, 
                                              levels = c("Yes", "No")))
forest_cm_default <- confusionMatrix(forest_table_default, positive = "Yes")
forest_cm_default
forest_cm_default$byClass[7]
```
Random Forest - Balanced Training Set <br/>
To try and improve the model, the balanced training set was used. Unlike the other improved models k-fold was not implemented due to computational restraints. The randomForest function requires the predictor variables to class match. Since the ordering was removed for ROSE it has to be removed for this test set to run.
```{r}
rf_model_bal <- randomForest(mental_health ~., data = train_set_bal, mtry = 6)

#Remove factor ordering for test set
test_set_bal <- test_set
test_set_bal$education <- factor(test_set_bal$education, ordered = FALSE)
test_set_bal$income <- factor(test_set_bal$income, ordered = FALSE)
test_set_bal$gen_health <- factor(test_set_bal$gen_health, ordered = FALSE)
test_set_bal$exercise_150 <- factor(test_set_bal$exercise_150, ordered = FALSE)

test_forest_pred <- predict(rf_model_bal, newdata = test_set_bal)
forest_table_bal <- table(Predicted = factor(test_forest_pred, 
                                             levels = c("Yes", "No")), 
                    Actual = factor(test_set$mental_health, 
                                    levels = c("Yes", "No")))
forest_cm_bal <- confusionMatrix(forest_table_bal, positive = "Yes")
forest_cm_bal
forest_cm_bal$byClass[7]
```
Comparing Random Forest <br/>
Using the balanced training set improved the model but the difference was not as large as some of the other models. This might be because Random Forest was more robust towards the original imbalance compared to other models. 
```{r}
kable(data.frame(Models = c("Default", "Balanced"),
                 'F1-score' = round(c(forest_cm_default$byClass[7], 
                                      forest_cm_bal$byClass[7]), 5),
                 Recall = round(c(forest_cm_default$byClass[6], 
                                  forest_cm_bal$byClass[6]), 5),
                 Precision = round(c(forest_cm_default$byClass[5], 
                                     forest_cm_bal$byClass[5]), 5),
                 'Detection Rate' = round(c(forest_cm_default$byClass[9], 
                                            forest_cm_bal$byClass[9]), 5),
                 'Balanced Accuracy' = round(c(forest_cm_default$byClass[11], 
                                               forest_cm_bal$byClass[11]), 5)),
      caption = "Random Forest",
      format = "simple")
```

Additional Look Into Random Forest <br/>
It was expected that Random Forest would outperform Decision Tree but the results from the separate training set shows Random Forest struggled at predicting new data. When looking at the confusion matrix from the second Random Forest model it shows that it was successful at building a classifier when trained on balanced data. It had a high F1-score with a close recall and precision measurement showing the classifier was making even false positives and false negative errors. Only when given the new and unbalanced test set, that all the models were compared with, did it have issues.
```{r}
confusionMatrix(rf_model_bal$confusion[-3, -3], positive = "Yes")
confusionMatrix(rf_model_bal$confusion[-3, -3], positive = "Yes")$byClass[7]
```

Naive Bayes - Default <br/>
The first Naive Bayes model was run with all the features and default split training set. The result had similar issues as the default Decision Tree. A low recall, high accuracy, and imbalanced dataset showed the Naive Bayes model also had a high rate of false negatives and missing at risk cases.
```{r}
train_NB_default <- naive_bayes(mental_health ~., data = train_set)
test_NB_pred <- predict(train_NB_default, newdata = test_set[, -1])
  #Response variable removed from test set in new data to avoid naive_bayes package warning
NB_table_default <- table(Predicted = factor(test_NB_pred, levels = c("Yes", "No")), 
                          Actual = factor(test_set$mental_health, levels = c("Yes", "No")))
NB_cm_default <- confusionMatrix(NB_table_default, positive = "Yes")
NB_cm_default
NB_cm_default$byClass[7]
```
Naive Bayes - Balanced Training Set and k-fold <br/>
This model used the balanced training dataset and 10-fold validation. The function used in this model is from 'caret' so k-fold could be implemented. Without k-fold being specified, caret's train function defaulted to bootstrapping and for this reason it was not used as the original default Naive Bayes model.
```{r}
train_NB_kf <- train(mental_health ~., data = train_set_bal, 
                     trControl = k_fold_train,
                     method = "naive_bayes")
test_NB_kf_pred <- predict(train_NB_kf, newdata = test_set)
NB_table_kf <- table(predicted = factor(test_NB_kf_pred, levels = c("Yes", "No")), 
                   actual = factor(test_set$mental_health, levels = c("Yes", "No")))
NB_cm_kf <- confusionMatrix(NB_table_kf, positive = "Yes")
NB_cm_kf
NB_cm_kf$byClass[7]
```
Comparing Naive Bayes Models <br/>
Since the Decision Tree model was the strongest using the balanced training set and k-fold no separate Naive Bayes with only the balanced training set was used. This had the smallest change and also had a drop in the F1-score. This was caused by the larger drop in precision relative to the increase in recall. 
```{r}
kable(data.frame(Models = c("Default", "Balanced and k-fold"),
                 'F1-score' = round(c(NB_cm_default$byClass[7], 
                                      NB_cm_kf$byClass[7]), 5),
                 Recall = round(c(NB_cm_default$byClass[6], 
                                  NB_cm_kf$byClass[6]), 5),
                 Precision = round(c(NB_cm_default$byClass[5], 
                                     NB_cm_kf$byClass[5]), 5),
                 'Detection Rate' = round(c(NB_cm_default$byClass[9], 
                                            NB_cm_kf$byClass[9]), 5),
                 'Balanced Accuracy' = round(c(NB_cm_default$byClass[11], 
                                               NB_cm_kf$byClass[11]), 5)),
      caption = "Naive Bayes",
      format = "simple")
```

Logistic Regression - Default <br/>
The first run trained the model with all the variables.
```{r}
train_log_default <- train(mental_health ~ ., data = train_set, method = "glm",
                           trControl = trainControl(method = "none"), 
                           family = "binomial")
test_log_pred <- predict(train_log_default, newdata = test_set)
log_table_default <- table(predicted = factor(test_log_pred, levels = c("Yes", "No")), 
                           actual = factor(test_set$mental_health, levels = c("Yes", "No")))
log_cm_default <- confusionMatrix(log_table_default, positive = "Yes")
log_cm_default
log_cm_default$byClass[7]
```
Logistic Regression - Balanced Training Set and k-fold <br/>


```{r}
train_log_kf <- train(mental_health ~ ., data = train_set_bal, method = "glm", 
                       trControl = k_fold_train, family = "binomial")
test_log_pred <- predict(train_log_kf, newdata = test_set)
log_table_kf <- table(Predicted = factor(test_log_pred, levels = c("Yes", "No")), 
                      Actual = factor(test_set$mental_health, levels = c("Yes", "No")))
log_cm_kf <- confusionMatrix(log_table_kf, positive = "Yes")
log_cm_kf
log_cm_kf$byClass[7]
```
Logistic Regression and Feature Selection <br/>
For additional exploration, stepwise regression using both backwards and forewards selection was run to see if there was a better formula for the logistic regression model. Interestingly the resulting function used all the features except for "children". In the earlier section with the correlation matrix "children" had a moderate negative correlation with "age". The "age" variable was deemed most important in the stepwise function as well as showing up as an earlier split in the Decision Tree, which made "children" an unimportant predictor. 

```{r}
full <- glm(mental_health ~ ., data = BRFSS, family = "binomial")
null <- glm(mental_health ~ 1, data = BRFSS, family = "binomial")
stepwise <- stepAIC(null, scope = list(lower = null, upper = full),
                direction = "both", trace = FALSE)
stepwise$formula

#Running Logistic Regression using the formula chosen by stepwise selection
train_log_step <- train(stepwise$formula, data = train_set_bal, method = "glm", 
                       trControl = k_fold_train, family = "binomial")
test_log_pred_step <- predict(train_log_step, newdata = test_set)
log_table_step <- table(predicted = factor(test_log_pred_step, 
                                           levels = c("Yes", "No")), 
                        actual = factor(test_set$mental_health, 
                                        levels = c("Yes", "No")))
log_cm_step <- confusionMatrix(log_table_step, positive = "Yes")
log_cm_step
log_cm_step$byClass[7]
```
Comparing Logistic Regression Models <br/>
The Logistic model had the biggest improvement when implementing k-fold and using the balanced training set. The formula generated by the stepwise function gave an almost identical result. 
```{r}
kable(data.frame(Models = c("Default", "Balanced & k-fold", "Stepwise"),
                 'F1-score' = round(c(log_cm_default$byClass[7], 
                                      log_cm_kf$byClass[7],
                                      log_cm_step$byClass[7]), 5),
                 Recall = round(c(log_cm_default$byClass[6], 
                                  log_cm_kf$byClass[6],
                                  log_cm_step$byClass[6]), 5),
                 Precision = round(c(log_cm_default$byClass[5], 
                                     log_cm_kf$byClass[5],
                                     log_cm_step$byClass[5]), 5),
                 'Detection Rate' = round(c(log_cm_default$byClass[9], 
                                            log_cm_kf$byClass[9],
                                            log_cm_step$byClass[9]), 5),
                 'Balanced Accuracy' = round(c(log_cm_default$byClass[11], 
                                               log_cm_kf$byClass[11],
                                               log_cm_step$byClass[11]), 5)),
      caption = "Logistic Regression",
      format = "simple")
```

Comparing All Models <br/>
The Decision Tree and Logistic Regression models performed the strongest. The surprise was that Random Forest did not out-perform Decision Tree. Since Random Forest uses multiple trees to create the model and Decision Tree only uses one it's possible that the seed just created a favourable single tree. 
```{r}
kable(data.frame(Models = c("Decision Tree - Balanced & k-fold", 
                            "Random Forest - Balanced",
                            "Naive Bayes - Balanced & k-fold",
                            "Logistic Regression - Balanced & k-fold"),
                 'F1-score' = round(c(tree_cm_kf$byClass[7], 
                                      forest_cm_bal$byClass[7],
                                      NB_cm_kf$byClass[7],
                                      log_cm_kf$byClass[7]), 5),
                 Recall = round(c(tree_cm_kf$byClass[6], 
                                  forest_cm_bal$byClass[6],
                                  NB_cm_kf$byClass[6],
                                  log_cm_kf$byClass[6]), 5),
                 Precision = round(c(tree_cm_kf$byClass[5], 
                                     forest_cm_bal$byClass[5],
                                     NB_cm_kf$byClass[5],
                                     log_cm_kf$byClass[5]), 5),
                 'Detection Rate' = round(c(tree_cm_kf$byClass[9], 
                                            forest_cm_bal$byClass[9],
                                            NB_cm_kf$byClass[9],
                                            log_cm_kf$byClass[9]), 5),
                 'Balanced Accuracy' = round(c(tree_cm_kf$byClass[11], 
                                               forest_cm_bal$byClass[11],
                                               NB_cm_kf$byClass[11],
                                               log_cm_kf$byClass[11]), 5)),
      caption = "Model Comparisons",
      format = "simple")
```

Comparing Variable Importance <br/>
The common variables that showed up in the ranking of important variables were: age, bad health days in the last 30 days, general health, and sex of the respondent. Except for Random Forest, every model ranked age as the most important. 
```{r}
#Decision Tree
varImp(train_tree_kf)

#Random Forest
varImpPlot(rf_model_bal)

#Naive Bayes
varImp(train_NB_kf)

#Logistic Regression
varImp(train_log_kf)
```


```{r}
model_names <- c("Tree", "Forest", "NB", "Log")
F1_height <- c(tree_cm_kf$byClass[7], forest_cm_bal$byClass[7],
               NB_cm_kf$byClass[7], log_cm_kf$byClass[7])
barplot(height = F1_height, names = model_names, main = "F1-scores", 
        ylim = c(0,0.6))

recall_height <- c(tree_cm_kf$byClass[6], forest_cm_bal$byClass[6],
                   NB_cm_kf$byClass[6], log_cm_kf$byClass[6])
barplot(height = recall_height, names = model_names, main = "Recall", 
        ylim = c(0,1))
```

